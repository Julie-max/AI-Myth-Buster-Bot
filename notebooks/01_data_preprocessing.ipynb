{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is for the data complexity labelling and preprocessing"
      ],
      "metadata": {
        "id": "oHS3FmUSFVzY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptdxhc-9aESF"
      },
      "outputs": [],
      "source": [
        "# Install missing dependency\n",
        "!pip install textstat datasets\n",
        "\n",
        "import json\n",
        "import random\n",
        "import pandas as pd\n",
        "import textstat  # Import textstat for readability scoring\n",
        "from datasets import Dataset, load_dataset\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define paths\n",
        "DATA_DIR = \"data\"\n",
        "RAW_DATA_PATH = os.path.join(DATA_DIR, \"ai_myths_expanded_1000.json\")\n",
        "PROCESSED_DATA_PATH = os.path.join(DATA_DIR, \"ai_myths_1000_complexity.json\")\n",
        "\n",
        "# Load the base dataset\n",
        "with open(RAW_DATA_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    dataset = json.load(f)\n",
        "\n",
        "# Process each example to compute Flesch Reading Ease score and assign Complexity label.\n",
        "# Higher readability score means easier (Simple); lower means more complex.\n",
        "for example in dataset:\n",
        "    readability = textstat.flesch_reading_ease(example[\"Debunked Fact\"])\n",
        "    example[\"Readability\"] = readability\n",
        "    example[\"Complexity\"] = \"Simple\" if readability >= 60 else \"Complex\"\n",
        "\n",
        "# Sort the dataset so that the easiest (highest readability) examples come first.\n",
        "dataset_sorted = sorted(dataset, key=lambda x: x[\"Readability\"], reverse=True)\n",
        "\n",
        "# Convert to Pandas DataFrame\n",
        "df = pd.DataFrame(dataset_sorted)\n",
        "\n",
        "# Save processed dataset\n",
        "df.to_json(PROCESSED_DATA_PATH, orient=\"records\", indent=4)\n",
        "\n",
        "print(f\"Dataset processed and saved at {PROCESSED_DATA_PATH}\")\n",
        "\n",
        "# Convert to Hugging Face Dataset\n",
        "dataset_hf = Dataset.from_pandas(df)"
      ],
      "metadata": {
        "id": "TtWpxQzSakqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Load Pre-trained T5 Model & Tokenizer\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\", model_max_length=128)\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
        "\n",
        "## Tokenize Data\n",
        "def preprocess_function(examples):\n",
        "    inputs = [\"debunk: \" + myth for myth in examples[\"Myth\"]]\n",
        "    targets = [fact for fact in examples[\"Debunked Fact\"]]\n",
        "\n",
        "    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\")\n",
        "    labels = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\").input_ids\n",
        "\n",
        "    model_inputs[\"labels\"] = [\n",
        "        [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels\n",
        "    ]\n",
        "\n",
        "    return model_inputs\n",
        "\n",
        "\n",
        "# Apply tokenization\n",
        "tokenized_datasets = dataset_hf.map(preprocess_function, batched=True)\n",
        "\n",
        "# Split the tokenized dataset into Simple and Complex subsets.\n",
        "simple_dataset = tokenized_datasets.select(range(500))\n",
        "complex_dataset = tokenized_datasets.select(range(500, 1000))"
      ],
      "metadata": {
        "id": "7vDL9lkNcWCq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}