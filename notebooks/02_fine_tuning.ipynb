{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is for fine tuning the model and training"
      ],
      "metadata": {
        "id": "PdbeZLUCFn1h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
        "from datasets import load_dataset, Dataset, concatenate_datasets\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "JcqF2vSmcoTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define paths\n",
        "DATA_DIR = \"data\"\n",
        "MODEL_DIR = \"models/t5_myth_buster\"\n",
        "PROCESSED_DATA_PATH = os.path.join(DATA_DIR, \"ai_myths_expanded_1000_complexity.json\")\n",
        "\n",
        "# Load Processed Dataset\n",
        "dataset = load_dataset(\"json\", data_files=PROCESSED_DATA_PATH)[\"train\"]\n",
        "\n",
        "## Load Pre-trained T5 Model & Tokenizer\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\", model_max_length=128)\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
        "\n",
        "## Tokenize Data\n",
        "def preprocess_function(examples):\n",
        "    inputs = [\"debunk: \" + myth for myth in examples[\"Myth\"]]\n",
        "    targets = [fact for fact in examples[\"Debunked Fact\"]]\n",
        "\n",
        "    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\")\n",
        "    labels = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\").input_ids\n",
        "\n",
        "    model_inputs[\"labels\"] = [\n",
        "        [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels\n",
        "    ]\n",
        "\n",
        "    return model_inputs\n",
        "\n",
        "\n",
        "# Apply tokenization\n",
        "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "# Split the tokenized dataset into Simple and Complex subsets.\n",
        "simple_dataset = tokenized_datasets.select(range(500))\n",
        "complex_dataset = tokenized_datasets.select(range(500, 1000))\n"
      ],
      "metadata": {
        "id": "3oK_0wQ-ctOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Stage 1: Fine-tune on the simple dataset\n",
        "training_args_stage1 = TrainingArguments(\n",
        "    output_dir=\"./results_stage1\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=3e-5,\n",
        "    per_device_train_batch_size=4,\n",
        "    num_train_epochs=7,\n",
        "    weight_decay=0.01,\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_dir=\"./logs_stage1\",\n",
        "    report_to=\"none\",\n",
        ")\n",
        "trainer_stage1 = Trainer(\n",
        "    model=model,\n",
        "    args=training_args_stage1,\n",
        "    train_dataset=simple_dataset,\n",
        "    eval_dataset=simple_dataset,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "trainer_stage1.train()\n",
        "\n",
        "# Stage 2: Fine-tune on a combined dataset (Simple + Complex)\n",
        "\n",
        "combined_dataset = concatenate_datasets([simple_dataset, complex_dataset])\n",
        "training_args_stage2 = TrainingArguments(\n",
        "    output_dir=\"./results_stage2\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=3e-5,\n",
        "    per_device_train_batch_size=4,\n",
        "    num_train_epochs=7,\n",
        "    weight_decay=0.01,\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_dir=\"./logs_stage2\",\n",
        "    report_to=\"none\",\n",
        ")\n",
        "trainer_stage2 = Trainer(\n",
        "    model=model,\n",
        "    args=training_args_stage2,\n",
        "    train_dataset=combined_dataset,\n",
        "    eval_dataset=combined_dataset,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "trainer_stage2.train()"
      ],
      "metadata": {
        "id": "TnrzEP7ZhoiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save final model & tokenizer\n",
        "model.save_pretrained(MODEL_DIR)\n",
        "tokenizer.save_pretrained(MODEL_DIR)\n",
        "print(f\"Model saved at {MODEL_DIR}\")"
      ],
      "metadata": {
        "id": "h80xIx22dQVJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}